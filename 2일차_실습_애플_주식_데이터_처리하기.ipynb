{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "2일차 실습: 애플 주식 데이터 처리하기",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYwHJBmiz6e5"
      },
      "source": [
        "애플 주식 데이터를 가지고 간단한 데이터 분석을 해보자. 모든 답은 Pyspark을 통해 이뤄져야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE0VhL0g1no8"
      },
      "source": [
        "먼저 PySpark과 Py4J를 설치하자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXgIyS_F0Kar",
        "outputId": "d0eee0d7-c203-4be4-f198-4aa675ee65fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9 "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 67kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=b8ccad5e081c40d1a78d4e748770dd716bbad5333cee2763060427a21ab3962a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNwc3F_Az6e6"
      },
      "source": [
        "#### Spark Session 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RveyavjYz6e7"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0DgR89Sz6e8"
      },
      "source": [
        "#### 애플 주식 CSV 파일 로딩하기: https://pyspark-test-sj.s3-us-west-2.amazonaws.com/appl_stock.csv\n",
        "일단 pandas 데이터프레임으로 로딩해서 Spark 데이터프레임으로 변경한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FeKmU3Piz6e8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "apple_pandas_df = pd.read_csv(\"https://pyspark-test-sj.s3-us-west-2.amazonaws.com/appl_stock.csv\")\n",
        "apple_spark_df = spark.createDataFrame(apple_pandas_df)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA3fbJ4Lz6e9"
      },
      "source": [
        "#### 1> 어떤 컬럼 이름들이 있는가?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b341_1Zfz6e9",
        "outputId": "c1942ef1-2e9d-4887-bbde-71194b0e9e9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "apple_spark_df.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXj2LCWuz6e_"
      },
      "source": [
        "#### 2> 스키마를 프린트해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQR5dwZjz6e_",
        "outputId": "f1bd366d-cdfd-4996-ee2b-ee90d34cee27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "apple_spark_df.printSchema()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: long (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFljhmp5z6fA"
      },
      "source": [
        "#### 3> 처음 5개의 레코드를 출력해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJQZ7PZDz6fA",
        "outputId": "e0ca9d5e-ae1f-46a8-ce1e-3cd850444845",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "apple_spark_df.show(n=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+----------+----------+----------+---------+------------------+\n",
            "|      Date|      Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
            "+----------+----------+----------+----------+----------+---------+------------------+\n",
            "|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|         27.727039|\n",
            "|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|         27.774976|\n",
            "|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178000000004|\n",
            "|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800|          27.28265|\n",
            "|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|         27.464034|\n",
            "+----------+----------+----------+----------+----------+---------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uZPeXHxz6fB"
      },
      "source": [
        "#### 4> describe를 사용하여 데이터프레임의 컬럼별 통계보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Apv3ZhW2Zhj",
        "outputId": "88a2bbd6-46ad-415d-e9bf-5d9dc17536b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "apple_spark_df.describe().show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+------------------+-----------------+------------------+------------------+--------------------+------------------+\n",
            "|summary|      Date|              Open|             High|               Low|             Close|              Volume|         Adj Close|\n",
            "+-------+----------+------------------+-----------------+------------------+------------------+--------------------+------------------+\n",
            "|  count|      1762|              1762|             1762|              1762|              1762|                1762|              1762|\n",
            "|   mean|      null|313.07631115890996|315.9112880164584|309.82824050794557| 312.9270656379114| 9.422577587968218E7| 75.00174115607273|\n",
            "| stddev|      null|185.29946803981545|186.8981768648577|183.38391664370988|185.14710361709427|6.0205187765927084E7|28.574929721799037|\n",
            "|    min|2010-01-04|              90.0|        90.699997|         89.470001|         90.279999|            11475900|         24.881912|\n",
            "|    max|2016-12-30|        702.409988|       705.070023|        699.569977|        702.100021|           470249500|        127.966091|\n",
            "+-------+----------+------------------+-----------------+------------------+------------------+--------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR1QL8-Z2auU"
      },
      "source": [
        "#### 5> Close 컬럼의 평균값은 얼마인가?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCQCMa0xz6fB",
        "outputId": "382f26e8-8d19-47e9-e6ad-57a1723297a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "apple_spark_df.select(mean(\"Close\")).show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|312.9270656379114|\n",
            "+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnE6Cbg_IONn"
      },
      "source": [
        "#### 6> Volume 컬럼의 최대값과 최소값은?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5mvFy0eIVPx",
        "outputId": "a21eac21-ecb2-49a6-cd6d-e061a6d4f745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "\n",
        "apple_spark_df.select(max(\"Volume\"), min(\"Volume\")).show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----------+\n",
            "|max(Volume)|min(Volume)|\n",
            "+-----------+-----------+\n",
            "|  470249500|   11475900|\n",
            "+-----------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax1Of8ATz6fD"
      },
      "source": [
        "#### 보너스 질문: HV ratio라는 이름의 새로운 컬럼을 추가한 데이터프레임을 만들기. 이 컬럼의 값은 High/Volume으로 계산된다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkO7rQ3Pz6fD"
      },
      "source": [
        "apple_spark_df_with_hv = apple_spark_df.withColumn(\"hv ratio\", apple_spark_df.High/apple_spark_df.Volume) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_bSkvOHEDC"
      },
      "source": [
        "apple_spark_df_with_hv.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIROQ8klz6fD"
      },
      "source": [
        "#### 보너스 질문: 월별 Close 컬럼의 평균값은?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_mr0fO_z6fD"
      },
      "source": [
        "from pyspark.sql.functions import month\n",
        "\n",
        "monthdf = apple_spark_df.withColumn(\"Month\", month(\"Date\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3mfqkYUHQ1e"
      },
      "source": [
        "monthavgdf = monthdf.select([\"Month\", \"Close\"]).groupBy(\"Month\").mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTpOy97YK_NC"
      },
      "source": [
        "monthavgdf.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mjKxisALA2t"
      },
      "source": [
        "monthavgdf.select([\"Month\", \"avg(Close)\"]).orderBy(\"Month\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5egHUUucLToO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}