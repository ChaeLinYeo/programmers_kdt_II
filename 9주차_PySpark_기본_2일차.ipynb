{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9주차_PySpark_기본_2일차.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learn-programmers/programmers_kdt_II/blob/main/9%EC%A3%BC%EC%B0%A8_PySpark_%EA%B8%B0%EB%B3%B8_2%EC%9D%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAqhTDfuWrcM"
      },
      "source": [
        "PySpark을 로컬머신에 설치하고 노트북을 사용하기 보다는 머신러닝 관련 다양한 라이브러리가 이미 설치되었고 좋은 하드웨어를 제공해주는 Google Colab을 통해 실습을 진행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIA23YgbXKJd"
      },
      "source": [
        "이를 위해 pyspark과 Py4J 패키지를 설치한다. Py4J 패키지는 파이썬 프로그램이 자바가상머신상의 오브젝트들을 접근할 수 있게 해준다. Local Standalone Spark을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbT0rpGfVdiq",
        "outputId": "825c3acf-372d-4033-d753-49010c2dab34"
      },
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9 "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 67kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 46.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=1ab6d28ae53ae0edaf7482161813b1135e98a3b423328132c43a9c9973b8c55e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQIB8vLPBMeu",
        "outputId": "f01cb8e7-db31-42ff-89d2-c8931c88b514"
      },
      "source": [
        "!ls -tl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Jan 20 17:27 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHXY3xYctlCe",
        "outputId": "4077e27f-9a1a-4745-ca07-f09494e83eec"
      },
      "source": [
        "!ls -tl sample_data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 55504\n",
            "-rw-r--r-- 1 root root 18289443 Jan 20 17:27 mnist_test.csv\n",
            "-rw-r--r-- 1 root root 36523880 Jan 20 17:27 mnist_train_small.csv\n",
            "-rw-r--r-- 1 root root   301141 Jan 20 17:27 california_housing_test.csv\n",
            "-rw-r--r-- 1 root root  1706430 Jan 20 17:27 california_housing_train.csv\n",
            "-rwxr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json\n",
            "-rwxr-xr-x 1 root root      930 Jan  1  2000 README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew_eTGrvXlDw"
      },
      "source": [
        "**Spark Session:** SparkSession은 Spark 2.0부터 엔트리 포인트로 사용된다. 그 이전에는 SparkContext가 사용되었다. SparkSession을 이용해 RDD, 데이터 프레임등을 만든다. SparkSession은 SparkSession.builder를 호출하여 생성하며 다양한 함수들을 통해 세부 설정이 가능하다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vm6tgcPXdnR"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# spark 세션 오브젝트 만들기\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\ # master다음의 인자는 내가 사용하고싶은 spark 클러스터 이름을 준다. local은 내 로컬 spark stanalone을 쓰겠다는 것. *은 서버의 모든 cpu를 쓴다는 것이다.\n",
        "        .appName('PySpark_Tutorial')\\\n",
        "        .getOrCreate() # PySpark_Tutorial와 같은거 있으면 불러오고 아니면 새로 만들어라"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "LSs_1PYaYWxI",
        "outputId": "e118a545-c64b-44a6-929a-f8971cbb2e2b"
      },
      "source": [
        "spark # 버전, 마스터는 로컬, 모든 cpu를 씀, 앱이름 출력"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c9c0e4674bec:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySpark_Tutorial</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f3fe18b15c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNHAAJuCZCqE"
      },
      "source": [
        "**Python 객체를 RDD로 변환해보기**\n",
        "\n",
        "**1> Python 리스트 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhijTd1UY1i9"
      },
      "source": [
        "name_list_json = [ '{\"name\": \"keeyong\"}', '{\"name\": \"benjamin\"}', '{\"name\": \"claire\"}' ]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXqf1lC_Zdxf",
        "outputId": "5d78a629-3f19-4403-91af-5aaf5e526376"
      },
      "source": [
        "for n in name_list_json: # 3개의 json 오브젝트 출력\n",
        "  print(n)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"name\": \"keeyong\"}\n",
            "{\"name\": \"benjamin\"}\n",
            "{\"name\": \"claire\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1_A6f6Nbr57",
        "outputId": "9d5a79f4-8694-4414-9872-d476225c6a5e"
      },
      "source": [
        "import json\n",
        "\n",
        "for n in name_list_json:\n",
        "  jn = json.loads(n) # 파이썬 딕셔너리로 바꾼 뒤\n",
        "  print(jn[\"name\"]) # 딕셔너리의 name만 출력"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keeyong\n",
            "benjamin\n",
            "claire\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKlanTznb75N"
      },
      "source": [
        "**2> 파이썬 리스트를 RDD로 변환. RDD로 변환되는 순간 Spark 클러스터의 서버들에 데이터가 나눠 저장됨 (파티션). 또한 Lazy Execution이 된다는 점 기억**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvTOpLsrZ_I8"
      },
      "source": [
        "rdd = spark.sparkContext.parallelize(name_list_json)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIMTLkPdk4ul",
        "outputId": "efaa0527-b93b-4ff6-ca0d-2065e30430c0"
      },
      "source": [
        "rdd"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmszVYtzaL3Q",
        "outputId": "dd7cd7c1-a7c2-4398-a9f8-b138b445c787"
      },
      "source": [
        "rdd.count() # rdd에 들어있는 레코드 수"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMLop2SQbMnZ"
      },
      "source": [
        "# rdd의 functional프로그래밍 map을 통해 해보기\n",
        "# map은 rdd 전체의 엘리먼트들에게 람다펑션을 취해서 새로운 rdd로 만든다.\n",
        "# string이었던 rdd의 원소들이 파이썬의 딕셔너리 형태로 바꿔서 parse_rdd에 저장된다.\n",
        "parsed_rdd = rdd.map(lambda el:json.loads(el))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAgNR7EWcl2t",
        "outputId": "4b7e9cf7-df83-4a42-da18-8f1d17f308af"
      },
      "source": [
        "parsed_rdd "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[3] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt5SMf4IcoIZ",
        "outputId": "93fc2dc3-2fe2-48a4-c50d-45925fa595ea"
      },
      "source": [
        "parsed_rdd.collect()\n",
        "# collect해서 출력하면 rdd의 정보들이 수집되어 파이썬 프로그램으로 넘어와 출력된다."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'keeyong'}, {'name': 'benjamin'}, {'name': 'claire'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Kmn_qycqQl"
      },
      "source": [
        "# rdd를 람다펑션으로 이름만 뽑도록 해보자.  \n",
        "parsed_name_rdd = rdd.map(lambda el:json.loads(el)[\"name\"])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZWxTKgJnDm8",
        "outputId": "7a7c74d6-9e2d-4ffb-d9a7-99bd634c2b54"
      },
      "source": [
        "parsed_name_rdd.collect() # 이름만 나오는 것을 확인할 수 있다.\n",
        "# 파이썬으로는 1000만개의 데이터에서 이름만 뽑을 수 없다.\n",
        "# 하지만 spark의 rdd를 이용해 1000만개더라도 이름만 뽑을 수 있다.\n",
        "# 다만 collect로 받아올 때는 받아올 데이터가 작아야 한다! 파이썬으로 넘겨오면서 오류날 수 있다."
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['keeyong', 'benjamin', 'claire']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McGKfLtWnsTB"
      },
      "source": [
        "**파이썬 리스트를 데이터프레임으로 변환하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WlhiRsKo7j7"
      },
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# name_list_json라는 파이썬 리스트를 이번에는 데이터프레임으로 바꾸자.\n",
        "# 스트링임을 알려줘야 한다. StringType()\n",
        "df = spark.createDataFrame(name_list_json, StringType())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2-pyiKGpLZe",
        "outputId": "c88aa689-9370-4a79-d999-69116c0864e6"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sql4KPhppgr",
        "outputId": "3188fc2c-669e-4c6e-9375-bffd2b08488c"
      },
      "source": [
        "df.printSchema()\n",
        "# rdd와 데이터프레임의 다른 점!\n",
        "# 컬럼이 생기고, 타입정보는 모르기때문에 위에서 StringType()로 명시한다.\n",
        "# 위에서 필드 이름을 안줬기 때문에 기본적으로 필드 이름이 value가 된다."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2XzTjdEpuZ4",
        "outputId": "ef32c8a0-c9ec-4488-8568-f8edec7f480c"
      },
      "source": [
        "df.select('*').collect()\n",
        "# 여러 필드 중 일부 필드만 선택해볼 수 있다.\n",
        "# *은 모든 필드 선택"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='{\"name\": \"keeyong\"}'),\n",
              " Row(value='{\"name\": \"benjamin\"}'),\n",
              " Row(value='{\"name\": \"claire\"}')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmLv4Guwny00",
        "outputId": "5072cc74-6e70-4651-c6aa-451b6a3c86c4"
      },
      "source": [
        "df.select('value').collect()\n",
        "# 여기서는 필드가 하나이기 때문에 위의 *를 한 결과와 동일하다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='{\"name\": \"keeyong\"}'),\n",
              " Row(value='{\"name\": \"benjamin\"}'),\n",
              " Row(value='{\"name\": \"claire\"}')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93TLUFxgqLrm"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# 필드(컬럼)에 이름을 지정해줄 수 있다.\n",
        "row = Row(\"name\") # Or some other column name\n",
        "\n",
        "# rdd의 경우 이름이 없기 때문에 row라는 모듈을 통해 이름을 주면서 데이터프레임으로 바꾼다.\n",
        "df_name = parsed_name_rdd.map(row).toDF()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtFEYRG61-L8",
        "outputId": "509a168e-5199-46a5-c625-4686a329974a"
      },
      "source": [
        "df_name.printSchema()\n",
        "# 필드 이름이 바뀐 것을 확인할 수 있다."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MptOIByKsLeG",
        "outputId": "dfd23c18-ccf2-4c5d-dbe3-b8834c0a4e55"
      },
      "source": [
        "df_name.select('name').collect()\n",
        "# name 필드의 내용을 볼 수 있다."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='keeyong'), Row(name='benjamin'), Row(name='claire')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaFoTe4eAIbI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}