{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-78fc683d5810>:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  enc.fit(y[:,np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-fd377ab0d802>:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  Y = enc.transform(y[:,np.newaxis]).toarray()\n"
     ]
    }
   ],
   "source": [
    "Y = enc.transform(y[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = X[:60000], X[60000:], Y[:60000], Y[60000:]\n",
    "\n",
    "# train, test, valid split\n",
    "# train 60%, test 20%, valid 20%\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "# X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = X[:42000], X[42000:56000], X[56000:], Y[:42000], Y[42000:56000], Y[56000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "X_valid = X_valid /255 # validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, 1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1)))\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization\n",
    "def compute_cost(X, T, W, lambd):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1) \n",
    "    cost = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1)) + (np.linalg.norm(W, ord=2)**2)*(lambd)/2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient 계산에 비용함수 반영\n",
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size, lambd):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch) + W*lambd)\n",
    "#         cost_history[i] = compute_cost(X_batch, T_batch, W, lambd)\n",
    "        if i % 1000 == 0:\n",
    "            cost_history = compute_cost(X_batch, T_batch, W, lambd)\n",
    "            print(cost_history)\n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda:  0.00969405027712516\n",
      "[[2.28089709]]\n",
      "[[0.64343334]]\n",
      "[[0.55232091]]\n",
      "[[0.42151459]]\n",
      "[[0.45281326]]\n",
      "[[0.30402932]]\n",
      "[[0.43888521]]\n",
      "[[0.44042627]]\n",
      "[[0.41963401]]\n",
      "[[0.37950515]]\n",
      "[[0.29549396]]\n",
      "[[0.33599236]]\n",
      "[[0.34745738]]\n",
      "[[0.38134414]]\n",
      "[[0.43862299]]\n",
      "[[0.44772082]]\n",
      "[[0.42055763]]\n",
      "[[0.37187197]]\n",
      "[[0.40307998]]\n",
      "[[0.27260287]]\n",
      "[[0.42571159]]\n",
      "[[0.43534326]]\n",
      "[[0.40387635]]\n",
      "[[0.37339211]]\n",
      "[[0.28664373]]\n",
      "[[0.30925039]]\n",
      "[[0.35719848]]\n",
      "[[0.3902503]]\n",
      "[[0.43025937]]\n",
      "[[0.44475086]]\n",
      "[[0.40246873]]\n",
      "[[0.38694756]]\n",
      "[[0.41061186]]\n",
      "[[0.28443305]]\n",
      "[[0.43273766]]\n",
      "[[0.44704317]]\n",
      "[[0.41131619]]\n",
      "[[0.38447308]]\n",
      "[[0.29629826]]\n",
      "[[0.30509371]]\n",
      "[[0.3674077]]\n",
      "[[0.40285636]]\n",
      "[[0.43405104]]\n",
      "[[0.44829322]]\n",
      "[[0.39998864]]\n",
      "[[0.40247323]]\n",
      "[[0.42151576]]\n",
      "[[0.29802961]]\n",
      "[[0.4404954]]\n",
      "[[0.4589101]]\n",
      "score:  0.9151428571428571\n",
      "lambda:  0.004591980349869061\n",
      "[[2.2844041]]\n",
      "[[0.50998123]]\n",
      "[[0.67913675]]\n",
      "[[0.4317515]]\n",
      "[[0.3675784]]\n",
      "[[0.40180991]]\n",
      "[[0.32177479]]\n",
      "[[0.45077918]]\n",
      "[[0.56383874]]\n",
      "[[0.37809819]]\n",
      "[[0.39765134]]\n",
      "[[0.25274229]]\n",
      "[[0.27220702]]\n",
      "[[0.46561674]]\n",
      "[[0.41585147]]\n",
      "[[0.25431476]]\n",
      "[[0.54880047]]\n",
      "[[0.36535482]]\n",
      "[[0.26269285]]\n",
      "[[0.35039422]]\n",
      "[[0.24380608]]\n",
      "[[0.40128716]]\n",
      "[[0.52274678]]\n",
      "[[0.36005724]]\n",
      "[[0.37394693]]\n",
      "[[0.22596321]]\n",
      "[[0.24317174]]\n",
      "[[0.43005572]]\n",
      "[[0.38909642]]\n",
      "[[0.24310469]]\n",
      "[[0.53499542]]\n",
      "[[0.35162866]]\n",
      "[[0.24644916]]\n",
      "[[0.33304991]]\n",
      "[[0.22786413]]\n",
      "[[0.39168083]]\n",
      "[[0.51194125]]\n",
      "[[0.35475634]]\n",
      "[[0.37164877]]\n",
      "[[0.22042448]]\n",
      "[[0.23610087]]\n",
      "[[0.41369738]]\n",
      "[[0.37930058]]\n",
      "[[0.24212284]]\n",
      "[[0.53018922]]\n",
      "[[0.34312249]]\n",
      "[[0.24347011]]\n",
      "[[0.32445431]]\n",
      "[[0.22419811]]\n",
      "[[0.38969818]]\n",
      "score:  0.9160714285714285\n",
      "lambda:  0.007367361110471329\n",
      "[[2.28162453]]\n",
      "[[0.54358179]]\n",
      "[[0.43498242]]\n",
      "[[0.49260803]]\n",
      "[[0.46298505]]\n",
      "[[0.40775758]]\n",
      "[[0.4682615]]\n",
      "[[0.31246602]]\n",
      "[[0.44805104]]\n",
      "[[0.41579645]]\n",
      "[[0.40518932]]\n",
      "[[0.22556478]]\n",
      "[[0.31273068]]\n",
      "[[0.33275894]]\n",
      "[[0.34660717]]\n",
      "[[0.2910448]]\n",
      "[[0.34948423]]\n",
      "[[0.44295758]]\n",
      "[[0.4182586]]\n",
      "[[0.39387945]]\n",
      "[[0.45388063]]\n",
      "[[0.28690793]]\n",
      "[[0.45132827]]\n",
      "[[0.45997618]]\n",
      "[[0.40071571]]\n",
      "[[0.21729628]]\n",
      "[[0.3090515]]\n",
      "[[0.3081364]]\n",
      "[[0.33410906]]\n",
      "[[0.27787078]]\n",
      "[[0.34636788]]\n",
      "[[0.43721607]]\n",
      "[[0.41236581]]\n",
      "[[0.4043656]]\n",
      "[[0.4597203]]\n",
      "[[0.29183052]]\n",
      "[[0.46234937]]\n",
      "[[0.48587499]]\n",
      "[[0.40552735]]\n",
      "[[0.22446608]]\n",
      "[[0.31511065]]\n",
      "[[0.30358994]]\n",
      "[[0.33395026]]\n",
      "[[0.27702227]]\n",
      "[[0.34858191]]\n",
      "[[0.43616288]]\n",
      "[[0.41027711]]\n",
      "[[0.41388639]]\n",
      "[[0.46796842]]\n",
      "[[0.2994603]]\n",
      "score:  0.9153571428571429\n",
      "lambda:  0.004911087378539255\n",
      "[[2.27993245]]\n",
      "[[0.62750427]]\n",
      "[[0.53030733]]\n",
      "[[0.5681299]]\n",
      "[[0.38061599]]\n",
      "[[0.42349237]]\n",
      "[[0.26437819]]\n",
      "[[0.33666774]]\n",
      "[[0.4651942]]\n",
      "[[0.36523499]]\n",
      "[[0.29556297]]\n",
      "[[0.36491898]]\n",
      "[[0.2915495]]\n",
      "[[0.28587268]]\n",
      "[[0.33407454]]\n",
      "[[0.39475979]]\n",
      "[[0.37944416]]\n",
      "[[0.45949278]]\n",
      "[[0.33484746]]\n",
      "[[0.35189075]]\n",
      "[[0.22544818]]\n",
      "[[0.32638796]]\n",
      "[[0.4184786]]\n",
      "[[0.32299495]]\n",
      "[[0.25379802]]\n",
      "[[0.33162698]]\n",
      "[[0.25947399]]\n",
      "[[0.26203834]]\n",
      "[[0.31202544]]\n",
      "[[0.38877697]]\n",
      "[[0.36353257]]\n",
      "[[0.43672242]]\n",
      "[[0.3469587]]\n",
      "[[0.33401253]]\n",
      "[[0.21908267]]\n",
      "[[0.33558411]]\n",
      "[[0.41454888]]\n",
      "[[0.31273203]]\n",
      "[[0.24550542]]\n",
      "[[0.32757708]]\n",
      "[[0.25629077]]\n",
      "[[0.2574472]]\n",
      "[[0.30452637]]\n",
      "[[0.39298567]]\n",
      "[[0.35930387]]\n",
      "[[0.42619996]]\n",
      "[[0.35777765]]\n",
      "[[0.32779288]]\n",
      "[[0.21811374]]\n",
      "[[0.34551154]]\n",
      "score:  0.9184285714285715\n",
      "lambda:  0.009250895365748631\n",
      "[[2.2747962]]\n",
      "[[0.58665299]]\n",
      "[[0.69383769]]\n",
      "[[0.38790619]]\n",
      "[[0.30200227]]\n",
      "[[0.43065717]]\n",
      "[[0.37869732]]\n",
      "[[0.39095995]]\n",
      "[[0.34566053]]\n",
      "[[0.28895784]]\n",
      "[[0.41387985]]\n",
      "[[0.53574808]]\n",
      "[[0.34545995]]\n",
      "[[0.39370017]]\n",
      "[[0.27674953]]\n",
      "[[0.37805618]]\n",
      "[[0.65493804]]\n",
      "[[0.31252576]]\n",
      "[[0.24042194]]\n",
      "[[0.41671075]]\n",
      "[[0.35052224]]\n",
      "[[0.34236499]]\n",
      "[[0.31576857]]\n",
      "[[0.27030901]]\n",
      "[[0.41761029]]\n",
      "[[0.55285023]]\n",
      "[[0.344748]]\n",
      "[[0.40769503]]\n",
      "[[0.28593252]]\n",
      "[[0.3685419]]\n",
      "[[0.66125849]]\n",
      "[[0.31174643]]\n",
      "[[0.24817034]]\n",
      "[[0.42187679]]\n",
      "[[0.36031436]]\n",
      "[[0.34526872]]\n",
      "[[0.31782201]]\n",
      "[[0.27600851]]\n",
      "[[0.42864675]]\n",
      "[[0.56756028]]\n",
      "[[0.35477984]]\n",
      "[[0.41850988]]\n",
      "[[0.30258373]]\n",
      "[[0.37023539]]\n",
      "[[0.66808869]]\n",
      "[[0.32000622]]\n",
      "[[0.2612532]]\n",
      "[[0.42686595]]\n",
      "[[0.37233745]]\n",
      "[[0.35517799]]\n",
      "score:  0.9126428571428571\n",
      "lambda:  0.004300000262001287\n",
      "[[2.28056696]]\n",
      "[[0.47905602]]\n",
      "[[0.38331602]]\n",
      "[[0.3852547]]\n",
      "[[0.33856065]]\n",
      "[[0.45987079]]\n",
      "[[0.4343076]]\n",
      "[[0.47850121]]\n",
      "[[0.26633717]]\n",
      "[[0.39050787]]\n",
      "[[0.22632812]]\n",
      "[[0.28783826]]\n",
      "[[0.31118594]]\n",
      "[[0.34154998]]\n",
      "[[0.45287118]]\n",
      "[[0.24940476]]\n",
      "[[0.27453448]]\n",
      "[[0.25819478]]\n",
      "[[0.26611194]]\n",
      "[[0.44285706]]\n",
      "[[0.36312056]]\n",
      "[[0.45806987]]\n",
      "[[0.2440134]]\n",
      "[[0.37343974]]\n",
      "[[0.19089126]]\n",
      "[[0.27684152]]\n",
      "[[0.29116173]]\n",
      "[[0.30152062]]\n",
      "[[0.44523996]]\n",
      "[[0.25850361]]\n",
      "[[0.27987353]]\n",
      "[[0.24074293]]\n",
      "[[0.25097744]]\n",
      "[[0.45101427]]\n",
      "[[0.3485818]]\n",
      "[[0.45868089]]\n",
      "[[0.24535481]]\n",
      "[[0.37669559]]\n",
      "[[0.18479003]]\n",
      "[[0.2770829]]\n",
      "[[0.28655671]]\n",
      "[[0.28627119]]\n",
      "[[0.44340593]]\n",
      "[[0.26911772]]\n",
      "[[0.28760354]]\n",
      "[[0.23534918]]\n",
      "[[0.2445993]]\n",
      "[[0.4616315]]\n",
      "[[0.34472012]]\n",
      "[[0.45970523]]\n",
      "score:  0.9132857142857143\n",
      "lambda:  0.009373240943413588\n",
      "[[2.27735509]]\n",
      "[[0.46935664]]\n",
      "[[0.49110751]]\n",
      "[[0.5077809]]\n",
      "[[0.56800818]]\n",
      "[[0.3665693]]\n",
      "[[0.40719226]]\n",
      "[[0.37873873]]\n",
      "[[0.2938974]]\n",
      "[[0.38894526]]\n",
      "[[0.46320856]]\n",
      "[[0.3013809]]\n",
      "[[0.26431974]]\n",
      "[[0.3337257]]\n",
      "[[0.38104766]]\n",
      "[[0.24333427]]\n",
      "[[0.34000275]]\n",
      "[[0.40821526]]\n",
      "[[0.48117413]]\n",
      "[[0.31045323]]\n",
      "[[0.38937207]]\n",
      "[[0.35470601]]\n",
      "[[0.26559017]]\n",
      "[[0.36020153]]\n",
      "[[0.44953163]]\n",
      "[[0.27722742]]\n",
      "[[0.26267683]]\n",
      "[[0.34179837]]\n",
      "[[0.37784242]]\n",
      "[[0.24525708]]\n",
      "[[0.34213388]]\n",
      "[[0.40689618]]\n",
      "[[0.46934797]]\n",
      "[[0.30598248]]\n",
      "[[0.39460519]]\n",
      "[[0.36489278]]\n",
      "[[0.27331097]]\n",
      "[[0.35155446]]\n",
      "[[0.45229485]]\n",
      "[[0.27888144]]\n",
      "[[0.27366109]]\n",
      "[[0.35558564]]\n",
      "[[0.38586857]]\n",
      "[[0.25664185]]\n",
      "[[0.35086871]]\n",
      "[[0.41271438]]\n",
      "[[0.46806408]]\n",
      "[[0.30969796]]\n",
      "[[0.40079176]]\n",
      "[[0.37717067]]\n",
      "score:  0.9073571428571429\n",
      "lambda:  0.0075996010955027395\n",
      "[[2.28075276]]\n",
      "[[0.55028966]]\n",
      "[[0.44019494]]\n",
      "[[0.37447984]]\n",
      "[[0.36097767]]\n",
      "[[0.32754099]]\n",
      "[[0.44798197]]\n",
      "[[0.41461808]]\n",
      "[[0.41914633]]\n",
      "[[0.44152223]]\n",
      "[[0.41705606]]\n",
      "[[0.31366924]]\n",
      "[[0.35249764]]\n",
      "[[0.32721029]]\n",
      "[[0.33144303]]\n",
      "[[0.36396118]]\n",
      "[[0.3766963]]\n",
      "[[0.2556143]]\n",
      "[[0.33464193]]\n",
      "[[0.27305563]]\n",
      "[[0.49448312]]\n",
      "[[0.35828252]]\n",
      "[[0.40771501]]\n",
      "[[0.45147226]]\n",
      "[[0.39202271]]\n",
      "[[0.30158087]]\n",
      "[[0.32371757]]\n",
      "[[0.31206445]]\n",
      "[[0.34908048]]\n",
      "[[0.36020357]]\n",
      "[[0.3809093]]\n",
      "[[0.24202919]]\n",
      "[[0.34760716]]\n",
      "[[0.2668939]]\n",
      "[[0.52218018]]\n",
      "[[0.35566718]]\n",
      "[[0.41546834]]\n",
      "[[0.46038116]]\n",
      "[[0.38512066]]\n",
      "[[0.30609973]]\n",
      "[[0.31312704]]\n",
      "[[0.31254292]]\n",
      "[[0.36513672]]\n",
      "[[0.36297449]]\n",
      "[[0.38639084]]\n",
      "[[0.24018295]]\n",
      "[[0.35984919]]\n",
      "[[0.26876936]]\n",
      "[[0.54012375]]\n",
      "[[0.36046744]]\n",
      "score:  0.9085\n",
      "lambda:  0.005665965618140807\n",
      "[[2.27920777]]\n",
      "[[0.47727248]]\n",
      "[[0.47469136]]\n",
      "[[0.40792339]]\n",
      "[[0.38934743]]\n",
      "[[0.47704954]]\n",
      "[[0.40692125]]\n",
      "[[0.44174651]]\n",
      "[[0.40625648]]\n",
      "[[0.70047432]]\n",
      "[[0.38376989]]\n",
      "[[0.23661598]]\n",
      "[[0.29820576]]\n",
      "[[0.43874547]]\n",
      "[[0.28521714]]\n",
      "[[0.27320087]]\n",
      "[[0.37434962]]\n",
      "[[0.30977396]]\n",
      "[[0.27560172]]\n",
      "[[0.38531657]]\n",
      "[[0.31589038]]\n",
      "[[0.36688169]]\n",
      "[[0.36674755]]\n",
      "[[0.69953]]\n",
      "[[0.3604032]]\n",
      "[[0.21057576]]\n",
      "[[0.2971251]]\n",
      "[[0.41882133]]\n",
      "[[0.29112015]]\n",
      "[[0.27331342]]\n",
      "[[0.36849784]]\n",
      "[[0.29781375]]\n",
      "[[0.25618879]]\n",
      "[[0.36009533]]\n",
      "[[0.29516838]]\n",
      "[[0.34490142]]\n",
      "[[0.35509792]]\n",
      "[[0.70092813]]\n",
      "[[0.35633588]]\n",
      "[[0.20477225]]\n",
      "[[0.30353559]]\n",
      "[[0.41411143]]\n",
      "[[0.29814214]]\n",
      "[[0.2795824]]\n",
      "[[0.36736449]]\n",
      "[[0.2959882]]\n",
      "[[0.25101899]]\n",
      "[[0.35043543]]\n",
      "[[0.28668213]]\n",
      "[[0.33647326]]\n",
      "score:  0.9195\n",
      "lambda:  0.0032413079285933876\n",
      "[[2.28526728]]\n",
      "[[0.53735878]]\n",
      "[[0.51240452]]\n",
      "[[0.27497729]]\n",
      "[[0.31507218]]\n",
      "[[0.19531712]]\n",
      "[[0.33479787]]\n",
      "[[0.44381479]]\n",
      "[[0.25092146]]\n",
      "[[0.27687465]]\n",
      "[[0.31991957]]\n",
      "[[0.22469476]]\n",
      "[[0.38392114]]\n",
      "[[0.43455409]]\n",
      "[[0.24978054]]\n",
      "[[0.22351561]]\n",
      "[[0.3201221]]\n",
      "[[0.19208361]]\n",
      "[[0.22386477]]\n",
      "[[0.1428665]]\n",
      "[[0.28640216]]\n",
      "[[0.36990101]]\n",
      "[[0.22345078]]\n",
      "[[0.22538115]]\n",
      "[[0.27645037]]\n",
      "[[0.19930089]]\n",
      "[[0.34807069]]\n",
      "[[0.41603814]]\n",
      "[[0.22765899]]\n",
      "[[0.21151189]]\n",
      "[[0.29058614]]\n",
      "[[0.18905404]]\n",
      "[[0.20486887]]\n",
      "[[0.14134621]]\n",
      "[[0.27005523]]\n",
      "[[0.3457706]]\n",
      "[[0.21860934]]\n",
      "[[0.20961354]]\n",
      "[[0.26124801]]\n",
      "[[0.19651884]]\n",
      "[[0.33204071]]\n",
      "[[0.41082439]]\n",
      "[[0.22135791]]\n",
      "[[0.21050333]]\n",
      "[[0.27752607]]\n",
      "[[0.18978064]]\n",
      "[[0.19582673]]\n",
      "[[0.14399254]]\n",
      "[[0.26088045]]\n",
      "[[0.33368093]]\n",
      "score:  0.918\n",
      "best score:  0.9195\n",
      "best lambda:  0.005665965618140807\n"
     ]
    }
   ],
   "source": [
    "# Regularization을 위한 가중치 lambda를 튜닝\n",
    "best_lambd = 0\n",
    "best_score = 0\n",
    "X = np.hstack((np.ones((np.size(X_valid, 0),1)),X_valid))\n",
    "T = y_valid\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "\n",
    "for lambd in range(10):\n",
    "    lambd = random.uniform(0,0.01)\n",
    "    print(\"lambda: \", lambd)\n",
    "    initial_cost = compute_cost(X, T, W, lambd)\n",
    "    (cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64, lambd)\n",
    "    \n",
    "    ## Accuracy\n",
    "    X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "    T_ = y_test\n",
    "    y_pred = predict(X_, W_optimal)\n",
    "    score = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_test))\n",
    "    \n",
    "    print(\"score: \", score)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_lambd = lambd\n",
    "print(\"best score: \", best_score)\n",
    "print(\"best lambda: \", best_lambd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
